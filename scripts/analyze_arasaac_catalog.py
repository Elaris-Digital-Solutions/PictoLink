"""
Script para analizar el cat√°logo ARASAAC y generar mapeos de categor√≠as
basados en las necesidades del usuario final.
"""

import json
from collections import Counter, defaultdict
from pathlib import Path
import re

# Definici√≥n de categor√≠as y palabras clave
CATEGORY_KEYWORDS = {
    'mas_usados': {
        'keywords': ['yo', 't√∫', 'hola', 'adi√≥s', 's√≠', 'no', 'querer', 'necesitar', 'ayuda', 'gracias'],
        'priority': 1,
        'limit': 25
    },
    'necesidades_basicas': {
        'keywords': ['hambre', 'sed', 'dolor', 'ayuda', 'cansado', 'sue√±o', 'dormir', 'descansar'],
        'priority': 1,
        'limit': 20
    },
    'ba√±o': {
        'keywords': ['ba√±o', 'inodoro', 'v√°ter', 'pa√±al', 'lavarse', 'ducha', 'jab√≥n', 'toalla', 'papel'],
        'priority': 1,
        'limit': 15
    },
    'emociones': {
        'keywords': ['feliz', 'triste', 'enojado', 'enfadado', 'asustado', 'sorprendido', 'contento', 'alegre', 'llorar', 're√≠r'],
        'priority': 1,
        'limit': 25
    },
    'saludos': {
        'keywords': ['hola', 'adi√≥s', 'buenos d√≠as', 'buenas tardes', 'buenas noches', 'gracias', 'por favor', 'perd√≥n'],
        'priority': 1,
        'limit': 12
    },
    'familia': {
        'keywords': ['mam√°', 'pap√°', 'madre', 'padre', 'hermano', 'hermana', 'abuelo', 'abuela', 't√≠o', 't√≠a', 'primo', 'beb√©'],
        'priority': 2,
        'limit': 20
    },
    'personas': {
        'keywords': ['ni√±o', 'ni√±a', 'beb√©', 'doctor', 'm√©dico', 'maestro', 'profesor', 'amigo', 'persona', 'hombre', 'mujer'],
        'priority': 2,
        'limit': 25
    },
    'comida': {
        'keywords': ['comer', 'beber', 'agua', 'pan', 'leche', 'fruta', 'manzana', 'pl√°tano', 'carne', 'pollo', 'arroz', 'pasta', 'sopa', 'jugo', 'caf√©'],
        'priority': 1,
        'limit': 40
    },
    'lugares': {
        'keywords': ['casa', 'escuela', 'colegio', 'parque', 'hospital', 'tienda', 'supermercado', 'iglesia', 'playa', 'ciudad'],
        'priority': 2,
        'limit': 25
    },
    'verbos': {
        'keywords': ['ir', 'venir', 'hacer', 'tener', 'querer', 'poder', 'comer', 'beber', 'dormir', 'jugar', 'ver', 'o√≠r', 'hablar', 'caminar', 'correr'],
        'priority': 1,
        'limit': 50
    },
    'deportes': {
        'keywords': ['f√∫tbol', 'baloncesto', 'nadar', 'nataci√≥n', 'correr', 'jugar', 'pelota', 'bal√≥n', 'deporte', 'gimnasia'],
        'priority': 3,
        'limit': 20
    },
    'animales': {
        'keywords': ['perro', 'gato', 'p√°jaro', 'pez', 'caballo', 'vaca', 'cerdo', 'oveja', 'conejo', 'rat√≥n', 'le√≥n', 'elefante'],
        'priority': 3,
        'limit': 30
    },
    'pronombres': {
        'keywords': ['yo', 't√∫', '√©l', 'ella', 'nosotros', 'vosotros', 'ellos', 'ellas', 'mi', 'tu', 'su'],
        'priority': 2,
        'limit': 10
    },
    'tiempo': {
        'keywords': ['lunes', 'martes', 'mi√©rcoles', 'jueves', 'viernes', 's√°bado', 'domingo', 'hoy', 'ma√±ana', 'ayer', 'd√≠a', 'noche', 'tarde', 'hora'],
        'priority': 2,
        'limit': 20
    },
    'transporte': {
        'keywords': ['coche', 'carro', 'auto', 'autob√∫s', 'bus', 'bicicleta', 'tren', 'avi√≥n', 'barco', 'moto'],
        'priority': 3,
        'limit': 15
    }
}

# IDs espec√≠ficos conocidos (de la configuraci√≥n actual)
KNOWN_IDS = {
    'mas_usados': [5584, 5596, 5441, 6653, 34567, 34568, 2275, 32464, 39122, 28669, 32301],
    'personas': [31807, 6997, 38961, 38962, 38963, 38964, 38965, 38966, 38968, 38969, 38970, 38971, 38972],
    'saludos': [34567, 34568, 6554, 6556, 6555, 6653, 6724],
    'necesidades_basicas': [39122, 32464, 28669, 32465, 32466, 2275, 32467, 32468, 32469],
    'emociones': [35545, 35546, 35547, 35548, 35549, 35550],
    'lugares': [6964, 6965, 6966, 6967, 6968, 6969],
    'verbos': [28669, 32465, 32466, 7004, 7007, 7006, 2345],
    'comida': [32464, 32470, 32471, 32472, 32473, 32474, 32475, 32476],
    'animales': [38967, 38968, 38969, 38970, 38971, 38972, 38973],
    'transporte': [6981, 6982, 6983, 6984, 6985, 6986],
    'ba√±o': [2275],  # Ba√±o/inodoro
}


def normalize_text(text):
    """Normaliza texto para comparaci√≥n"""
    if not text:
        return ""
    text = text.lower().strip()
    # Remover acentos
    replacements = {
        '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
        '√±': 'n', '√º': 'u'
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text


def matches_keywords(label, keywords):
    """Verifica si una etiqueta coincide con alguna palabra clave"""
    normalized_label = normalize_text(label)
    for keyword in keywords:
        normalized_keyword = normalize_text(keyword)
        if normalized_keyword in normalized_label or normalized_label in normalized_keyword:
            return True
    return False


def analyze_catalog(catalog_path):
    """
    Analiza el cat√°logo ARASAAC completo
    """
    print(f"üìä Analizando cat√°logo: {catalog_path}")
    
    # Estad√≠sticas generales
    total_pictograms = 0
    categories_native = Counter()
    
    # Mapeo a nuevas categor√≠as
    category_mappings = defaultdict(list)
    
    # Leer cat√°logo l√≠nea por l√≠nea
    with open(catalog_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue
                
            try:
                picto = json.loads(line)
                total_pictograms += 1
                
                picto_id = picto.get('id')
                label_es = picto.get('labels', {}).get('es', '')
                
                if not label_es:
                    continue
                
                # Analizar categor√≠as nativas de ARASAAC
                native_cats = picto.get('sources', {}).get('es', {}).get('raw', {}).get('categories', [])
                for cat in native_cats:
                    categories_native[cat] += 1
                
                # Mapear a nuevas categor√≠as basadas en keywords
                for category, config in CATEGORY_KEYWORDS.items():
                    keywords = config['keywords']
                    limit = config['limit']
                    
                    # Verificar si ya tenemos suficientes pictogramas
                    if len(category_mappings[category]) >= limit:
                        continue
                    
                    # Primero agregar IDs conocidos
                    if picto_id in KNOWN_IDS.get(category, []):
                        if picto_id not in [p['id'] for p in category_mappings[category]]:
                            category_mappings[category].append({
                                'id': picto_id,
                                'label': label_es,
                                'source': 'known_id'
                            })
                        continue
                    
                    # Luego buscar por keywords
                    if matches_keywords(label_es, keywords):
                        if picto_id not in [p['id'] for p in category_mappings[category]]:
                            category_mappings[category].append({
                                'id': picto_id,
                                'label': label_es,
                                'source': 'keyword_match'
                            })
                
                # Progreso cada 1000 pictogramas
                if line_num % 1000 == 0:
                    print(f"  Procesados: {line_num} pictogramas...")
                    
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  Error en l√≠nea {line_num}: {e}")
                continue
    
    print(f"\n‚úÖ An√°lisis completado!")
    print(f"   Total de pictogramas: {total_pictograms}")
    print(f"   Categor√≠as nativas encontradas: {len(categories_native)}")
    
    return category_mappings, categories_native


def generate_report(category_mappings, categories_native, output_dir):
    """
    Genera reportes del an√°lisis
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Reporte de categor√≠as nativas
    print("\nüìù Generando reporte de categor√≠as nativas...")
    native_report_path = output_dir / "native_categories_report.txt"
    with open(native_report_path, 'w', encoding='utf-8') as f:
        f.write("CATEGOR√çAS NATIVAS DE ARASAAC\n")
        f.write("=" * 50 + "\n\n")
        for cat, count in categories_native.most_common(50):
            f.write(f"{cat}: {count} pictogramas\n")
    print(f"   ‚úÖ Guardado en: {native_report_path}")
    
    # Reporte de mapeo a nuevas categor√≠as
    print("\nüìù Generando reporte de nuevas categor√≠as...")
    mapping_report_path = output_dir / "category_mappings_report.txt"
    with open(mapping_report_path, 'w', encoding='utf-8') as f:
        f.write("MAPEO A NUEVAS CATEGOR√çAS\n")
        f.write("=" * 50 + "\n\n")
        
        for category in sorted(category_mappings.keys()):
            pictos = category_mappings[category]
            config = CATEGORY_KEYWORDS[category]
            
            f.write(f"\n{category.upper()} (Prioridad: {config['priority']}, L√≠mite: {config['limit']})\n")
            f.write("-" * 50 + "\n")
            f.write(f"Pictogramas encontrados: {len(pictos)}\n\n")
            
            for i, picto in enumerate(pictos[:20], 1):  # Mostrar primeros 20
                f.write(f"  {i}. ID {picto['id']}: {picto['label']} ({picto['source']})\n")
            
            if len(pictos) > 20:
                f.write(f"  ... y {len(pictos) - 20} m√°s\n")
    
    print(f"   ‚úÖ Guardado en: {mapping_report_path}")
    
    # Generar archivo JSON para usar en la aplicaci√≥n
    print("\nüìù Generando archivo JSON de categor√≠as...")
    json_output_path = output_dir / "category_mappings.json"
    
    # Convertir a formato simple (solo IDs)
    simple_mappings = {}
    for category, pictos in category_mappings.items():
        simple_mappings[category] = [p['id'] for p in pictos]
    
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(simple_mappings, f, indent=2, ensure_ascii=False)
    
    print(f"   ‚úÖ Guardado en: {json_output_path}")
    
    # Resumen en consola
    print("\n" + "=" * 60)
    print("RESUMEN DE CATEGOR√çAS")
    print("=" * 60)
    for category in sorted(category_mappings.keys(), key=lambda x: CATEGORY_KEYWORDS[x]['priority']):
        pictos = category_mappings[category]
        config = CATEGORY_KEYWORDS[category]
        priority_stars = "‚≠ê" * config['priority']
        coverage = len(pictos) / config['limit'] * 100
        
        print(f"{priority_stars} {category:20s}: {len(pictos):3d}/{config['limit']:3d} ({coverage:5.1f}%)")


def main():
    """
    Funci√≥n principal
    """
    print("\n" + "=" * 60)
    print("AN√ÅLISIS DE CAT√ÅLOGO ARASAAC")
    print("Generaci√≥n de Categor√≠as Optimizadas para Usuarios")
    print("=" * 60 + "\n")
    
    # Rutas
    catalog_path = Path("nlp_backend/data/arasaac_catalog.jsonl")
    output_dir = Path("analysis_results")
    
    if not catalog_path.exists():
        print(f"‚ùå Error: No se encontr√≥ el cat√°logo en {catalog_path}")
        return
    
    # Ejecutar an√°lisis
    category_mappings, categories_native = analyze_catalog(catalog_path)
    
    # Generar reportes
    generate_report(category_mappings, categories_native, output_dir)
    
    print("\n‚úÖ Proceso completado exitosamente!")
    print(f"üìÅ Resultados guardados en: {output_dir}/")


if __name__ == "__main__":
    main()
